{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Causal Discovery Algorithms on Artificial Data\n",
    "\n",
    "Pipeline:\n",
    "- generate artificial time series data from causal networks (Bayesian Networks)\n",
    "- apply causal discovery algorithms (PCMCI ParCorr & Granger Causality)\n",
    "- evaluate results: how many of the causal links were detected? + precision, recall, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# time series generation from Bayesian Networks\n",
    "from tsBNgen import *\n",
    "from tsBNgen.tsBNgen import *\n",
    "\n",
    "# causal discovery method PCMCI\n",
    "import tigramite\n",
    "from tigramite import data_processing as pp\n",
    "from tigramite import plotting as tp\n",
    "from tigramite.pcmci import PCMCI\n",
    "from tigramite.independence_tests import ParCorr, GPDC, CMIknn, CMIsymb\n",
    "\n",
    "# granger causality\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample graph from tsBNgen - architecture 1\n",
    "\n",
    "# parameters\n",
    "T=20 # length of time series\n",
    "N=2000 # number of examples\n",
    "N_level=[2,4] # Number of possible levels for discrete nodes\n",
    "Mat=pd.DataFrame(np.array(([0,1,1],[0,0,1],[0,0,0]))) # adjacency matrix\n",
    "Node_Type=['D','D','C'] # for each node, specify if it is continuous or discrete\n",
    "CPD={'0':[0.6,0.4],'01':[[0.5,0.3,0.15,0.05],[0.1,0.15,0.3,0.45]],'012':{'mu0':10,'sigma0':2,'mu1':30,'sigma1':5,\n",
    "    'mu2':50,'sigma2':5,'mu3':70,'sigma3':5,'mu4':15,'sigma4':5,'mu5':50,'sigma5':5,'mu6':70,'sigma6':5,'mu7':90,'sigma7':3\n",
    "}} # probability distributions\n",
    "Parent={'0':[],'1':[0],'2':[0,1]} # parents at t=0\n",
    "\n",
    "CPD2={'00':[[0.7,0.3],[0.2,0.8]],'011':[[0.7,0.2,0.1,0],[0.6,0.3,0.05,0.05],[0.35,0.5,0.15,0],\n",
    "[0.2,0.3,0.4,0.1],[0.3,0.3,0.2,0.2],[0.1,0.2,0.3,0.4],[0.05,0.15,0.3,0.5],[0,0.05,0.25,0.7]],'012':{'mu0':10,'sigma0':2,'mu1':30,'sigma1':5,\n",
    "    'mu2':50,'sigma2':5,'mu3':70,'sigma3':5,'mu4':15,'sigma4':5,'mu5':50,'sigma5':5,'mu6':70,'sigma6':5,'mu7':90,'sigma7':3\n",
    "}}\n",
    "\n",
    "Parent2={'0':[0],'1':[0,1],'2':[0,1]} # parents at t=1\n",
    "loopbacks={'00':[1],'11':[1]} # self-loops of lag=1\n",
    "\n",
    "# generate time series\n",
    "Time_series1=tsBNgen(T, N, N_level, Mat, Node_Type, CPD, Parent, CPD2, Parent2, loopbacks)\n",
    "Time_series1.BN_data_gen()\n",
    "\n",
    "# get the corresponding variable names\n",
    "var_names = [key for key in Time_series1.BN_Nodes.keys()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True links: \n",
      "Variable 0 significant links:\n",
      "0 (1)\n",
      "Variable 1 significant links:\n",
      "0 (0)\n",
      "1 (1)\n",
      "Variable 2 significant links:\n",
      "0 (0)\n",
      "1 (0)\n"
     ]
    }
   ],
   "source": [
    "# set up true matrix:\n",
    "# adjacency matrix (row --> column)\n",
    "self_loops = np.array(([1,0,0],[0,1,0],[0,0,0]))\n",
    "actual_links = np.stack((Mat.copy(), self_loops), axis=2)\n",
    "\n",
    "# print the true links:\n",
    "print(\"True links: \")\n",
    "for i, var in enumerate(var_names):\n",
    "    print(\"Variable \"+ str(var)+ \" significant links:\")\n",
    "    for j, var2 in enumerate(var_names):\n",
    "        for k in np.arange(actual_links.shape[2]):\n",
    "            if actual_links[j,i,k]==1:\n",
    "                print(str(var2)+\" (\"+str(k)+\")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape time series (for use in causal discovery)\n",
    "# want shape ( examples*time , features ) = ( N*T , Mat.shape[1] )\n",
    "ts1 = np.array([np.array(Time_series1.BN_Nodes[key]).flatten() for key in Time_series1.BN_Nodes]).T\n",
    "# time vector\n",
    "ts1_time = np.array(list(np.arange(0,T))*N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Significant links at alpha = 0.05:\n",
      "\n",
      "    Variable 0 has 3 link(s):\n",
      "        (2  0): pval = 0.00000 | qval = 0.00000 | val =  0.709\n",
      "        (1  0): pval = 0.00000 | qval = 0.00000 | val =  0.549\n",
      "        (0 -1): pval = 0.00000 | qval = 0.00000 | val =  0.429\n",
      "\n",
      "    Variable 1 has 5 link(s):\n",
      "        (2  0): pval = 0.00000 | qval = 0.00000 | val =  0.959\n",
      "        (0  0): pval = 0.00000 | qval = 0.00000 | val =  0.549\n",
      "        (0 -1): pval = 0.00000 | qval = 0.00000 | val =  0.121\n",
      "        (1 -1): pval = 0.00000 | qval = 0.00000 | val =  0.049\n",
      "        (2 -1): pval = 0.00000 | qval = 0.00000 | val =  0.027\n",
      "\n",
      "    Variable 2 has 5 link(s):\n",
      "        (1  0): pval = 0.00000 | qval = 0.00000 | val =  0.959\n",
      "        (0  0): pval = 0.00000 | qval = 0.00000 | val =  0.709\n",
      "        (0 -1): pval = 0.00000 | qval = 0.00000 | val =  0.157\n",
      "        (1 -1): pval = 0.00000 | qval = 0.00000 | val =  0.034\n",
      "        (2 -1): pval = 0.00000 | qval = 0.00000 | val =  0.030\n"
     ]
    }
   ],
   "source": [
    "tau_max = 1\n",
    "tau_min = 0\n",
    "alpha_level = 0.05\n",
    "\n",
    "## METHOD 1  - PCMCI ParCorr\n",
    "var_names = [key for key in Time_series1.BN_Nodes.keys()]\n",
    "\n",
    "dataframe = pp.DataFrame(ts1, datatime = ts1_time ,var_names=var_names)\n",
    "\n",
    "parcorr = ParCorr(significance='analytic')\n",
    "pcmci = PCMCI(dataframe=dataframe, cond_ind_test=parcorr)\n",
    "\n",
    "pcmci.verbosity = 0\n",
    "results = pcmci.run_pcmci(tau_max=tau_max, tau_min=tau_min, pc_alpha=None)\n",
    "\n",
    "# corrected p-values\n",
    "q_matrix = pcmci.get_corrected_pvalues(p_matrix=results['p_matrix'], fdr_method='fdr_bh')\n",
    "# significant links at alpha = 0.01\n",
    "pcmci.print_significant_links(p_matrix = results['p_matrix'], \n",
    "                              q_matrix = q_matrix,\n",
    "                              val_matrix = results['val_matrix'],\n",
    "                              alpha_level = alpha_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcorr_sig = (q_matrix<alpha_level).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# PCMCI GPDC\n",
    "gpdc = GPDC(significance='analytic', gp_params=None)\n",
    "pcmci_gpdc = PCMCI(dataframe=dataframe, cond_ind_test=gpdc, verbosity=0)\n",
    "\n",
    "results_gpdc = pcmci_gpdc.run_pcmci(tau_max=tau_max, tau_min=tau_min, pc_alpha=0.05)\n",
    "\n",
    "\n",
    "# corrected p-values\n",
    "q_matrix_gpdc = pcmci_gpdc.get_corrected_pvalues(p_matrix=results_gpdc['p_matrix'], fdr_method='fdr_bh')\n",
    "# significant links at alpha = 0.01\n",
    "pcmci_gpdc.print_significant_links(p_matrix = results_gpdc['p_matrix'], \n",
    "                              q_matrix = q_matrix_gpdc,\n",
    "                              val_matrix = results_gpdc['val_matrix'],\n",
    "                              alpha_level = 0.01)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 2 - Granger Causality\n",
    "# prepare output array (rows & cols are variables, 3rd dimension are lags)\n",
    "grang = np.zeros([ts1.shape[1], \n",
    "                 ts1.shape[1],\n",
    "                 tau_max+1])\n",
    "# compute p-values of Granger causality test\n",
    "# using ssr_ftest\n",
    "for i in range(0,ts1.shape[1]):\n",
    "    for j in range(0,ts1.shape[1]):\n",
    "        tempg = grangercausalitytests(ts1[:,[i,j]], maxlag=tau_max+1, verbose=False)\n",
    "        # store the p-value result for the ssr f-test:\n",
    "        out = [tempg[x][0]['ssr_ftest'][1] for x in range(1,tau_max+1)]\n",
    "        grang[i,j,0:tau_max] = np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Significant links at alpha = 0.05\n",
      "Variable 0 significant links:\n",
      "Variable 1 significant links:\n",
      "0 (0)\n",
      "2 (0)\n",
      "Variable 2 significant links:\n",
      "0 (0)\n",
      "1 (0)\n"
     ]
    }
   ],
   "source": [
    "alpha_level=0.05\n",
    "# print sig links:\n",
    "print(\"Significant links at alpha = \"+str(alpha_level))\n",
    "for i, var in enumerate(var_names):\n",
    "    print(\"Variable \"+ str(var)+ \" significant links:\")\n",
    "    for j, var2 in enumerate(var_names):\n",
    "        for k in np.arange(0,tau_max):\n",
    "            if grang[i,j,k]<alpha_level:\n",
    "                print(str(var2)+\" (\"+str(k)+\")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "grang_sig = (grang<alpha_level).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcorr</th>\n",
       "      <th>granger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true positive rate</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     parcorr   granger\n",
       "accuracy            0.555556  0.333333\n",
       "precision           0.384615  0.230769\n",
       "recall              1.000000  0.600000\n",
       "true positive rate  1.000000  0.600000"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate results\n",
    "# parcorr\n",
    "accuracy_parcorr = accuracy_score(y_true = actual_links.flatten(), y_pred=parcorr_sig.flatten(), normalize=True, sample_weight=None)\n",
    "precision_parcorr = precision_score(y_true = actual_links.flatten(), y_pred=parcorr_sig.flatten(), labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
    "recall_parcorr = recall_score(y_true = actual_links.flatten(), y_pred=parcorr_sig.flatten(), labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
    "tp_parcorr = np.sum(actual_links*parcorr_sig) / np.sum(actual_links)\n",
    "# granger\n",
    "accuracy_grang = accuracy_score(y_true = actual_links.flatten(), y_pred=grang_sig.flatten(), normalize=True, sample_weight=None)\n",
    "precision_grang = precision_score(y_true = actual_links.flatten(), y_pred=grang_sig.flatten(), labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
    "recall_grang = recall_score(y_true = actual_links.flatten(), y_pred=grang_sig.flatten(), labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
    "tp_grang = np.sum(actual_links*grang_sig) / np.sum(actual_links)\n",
    "\n",
    "# output\n",
    "pd.DataFrame({'parcorr':[accuracy_parcorr, precision_parcorr, recall_parcorr, tp_parcorr],\n",
    "             'granger':[accuracy_grang, precision_grang, recall_grang, tp_grang],\n",
    "             }, index=['accuracy', 'precision','recall', 'true positive rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
